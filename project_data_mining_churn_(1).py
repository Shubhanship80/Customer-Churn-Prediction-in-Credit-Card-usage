# -*- coding: utf-8 -*-
"""Project_Data_Mining_Churn_(1).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1eSHrbqrizYr21ZJQBTPazEk-BpcM3N7E
"""

import pandas as pd
import seaborn as sns
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import confusion_matrix, classification_report, roc_auc_score, roc_curve
import statsmodels.api as sm
from sklearn.neighbors import KNeighborsClassifier
from sklearn.naive_bayes import MultinomialNB
from sklearn.neural_network import MLPClassifier
from sklearn.preprocessing import MinMaxScaler
import tensorflow as tf
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense,Activation,Dropout
from tensorflow.keras.callbacks import EarlyStopping

from google.colab import files
uploaded = files.upload()

import io
df = pd.read_csv(io.BytesIO(uploaded['ChurnP.csv']))

df.shape

df.head()

df['country'].value_counts()

#Dropping Variables with Many Levels - Customer_id- and Variable with variance 0 - Country-
df = df.drop(['customer_id' ], axis = 1)

df.shape

df.info()

#Converting some numerical variables to categorical variables to get them dummy
df['credit_card'] =  df['credit_card'].astype('category',copy=False)
df['active_member'] =  df['active_member'].astype('category',copy=False)
df['churn'] =  df['churn'].astype('category',copy=False)

df.info()

df.describe()

#get dummies for categorical variables
df1 = pd.get_dummies(df)
#df1 = pd.get_dummies(df, drop_first=True)

df1.head()

sns.heatmap(df.isnull(), cbar=True)

sns.pairplot(df, hue = 'churn')

df1.corr()['churn_1'].sort_values(ascending=False)

plt.figure(figsize = (16,5))
sns.heatmap(df.corr(), annot=True, linewidths=.5)

"""Visualization

Describing the data set
"""

sns.set(style="darkgrid")
ax = sns.countplot(x='churn', data = df, palette = "Set2")
#bar_container= ax.bar(df['churn'].value_counts().index, df['churn'].value_counts().values)
ax.bar_label(bar_container, fmt='{:,.0f}') #crear etiquetas en las barras
ax.set_title('Proportion of the target variable Churn')
plt.show()

sns.set(style="darkgrid")
ax = sns.countplot(x='country', hue = 'churn', data = df, palette = "Set2")
ax.set_title('Proportion of the target variable Churn by Country')
plt.show()

fig1, ax1 = plt.subplots()
x = list(df['gender'].value_counts().index)
y = list(df['gender'].value_counts())
ax1.pie(y, labels = x, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
ax1.set_title('Proportion of gender of the dataset')
plt.show()
print(df['gender'].value_counts())

df['churn'].value_counts()

churn= df[df['churn'] == 1]

fig1, ax1 = plt.subplots()
x = list(churn['gender'].value_counts().index)
y = list(churn['gender'].value_counts())
ax1.pie(y, labels = x, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
ax1.set_title('Proportion of gender who churn')
plt.show()
print(churn['gender'].value_counts())

ax = sns.boxplot(y = "age", x = 'churn',
                 palette = "cubehelix", data = df)
ax.set_title('Mean age of people in the dataset')

fig1, ax1 = plt.subplots()
x = list(df['credit_card'].value_counts().index)
y = list(df['credit_card'].value_counts())
ax1.pie(y, labels = x, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
ax1.set_title('Proportion Credit Card Holder')
plt.show()
print(df['credit_card'].value_counts())

fig1, ax1 = plt.subplots()
x = list(churn['credit_card'].value_counts().index)
y = list(churn['credit_card'].value_counts())
ax1.pie(y, labels = x, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
ax1.set_title('Proportion of credit card holder who churn')
plt.show()
print(churn['credit_card'].value_counts())

fig1, ax1 = plt.subplots()
x = list(df['active_member'].value_counts().index)
y = list(df['active_member'].value_counts())
ax1.pie(y, labels = x, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
ax1.set_title('Proportion of Active Member')
plt.show()
print(df['credit_card'].value_counts())

fig1, ax1 = plt.subplots()
x = list(churn['active_member'].value_counts().index)
y = list(churn['active_member'].value_counts())
ax1.pie(y, labels = x, autopct='%1.1f%%', startangle=90)
ax1.axis('equal')
ax1.set_title('Proportion of Active Member who churn')
plt.show()
print(churn['active_member'].value_counts())

ax = sns.distplot(df['balance'], bins = 15)
ax.set_title('Distribution of the Balance')
plt.show()
print('Average Balance =', df['balance'].mean())

ax = sns.distplot(churn['balance'], bins = 15)
ax.set_title('Distribution of the Balance who churn')
plt.show()
print('Average Balance =', churn['balance'].mean())

ax = sns.distplot(df['estimated_salary'], bins = 15)
ax.set_title('Distribution of Estimated Salary')
plt.show()
print('Average Estimated Salary =', df['estimated_salary'].mean())

ax = sns.distplot(churn['estimated_salary'], bins = 15)
ax.set_title('Distribution of Estimated Salary who churn')
plt.show()
print('Average Estimated Salary =', churn['estimated_salary'].mean())

sns.set(style="darkgrid")
ax = sns.countplot(x='tenure', hue = 'churn', data = df, palette = "Set3")
ax.set_title('Proportion of product by No Churn and Churn')
plt.show()

ax = sns.boxplot(y = "credit_score", x = 'churn',
                 palette = "cubehelix", data = df)
ax.set_title('Average Credit Score of the data set No Churn and Churn')

sns.set(style="darkgrid")
ax = sns.countplot(x='products_number', hue = 'churn', data = df, palette = "Set1")
ax.set_title('Proportion of product by No Churn and Churn')
plt.show()

df1=df1.drop('churn_0', axis=1)

df1.columns

#logistic regression v1

scaler = MinMaxScaler()
XS = scaler.fit_transform(df1)
X1 = pd.DataFrame(XS, columns= df1.columns)

X = X1.drop('churn_1', axis = 1)
y = X1['churn_1']
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)
logmodel = LogisticRegression(solver = 'liblinear')
logmodel.fit(train_X, train_y)
y_pred= logmodel.predict(test_X)
print(confusion_matrix(test_y,y_pred))
print(classification_report(test_y,y_pred))
print('ROC AUC VALUE: ', roc_auc_score(test_y,logmodel.predict_proba(test_X)[:,1]))
logit_model=sm.Logit(train_y, train_X)
logmodel_2=logit_model.fit()
print(logmodel_2.summary2())

#logistic regression v2 dropping variables
df2 = df1[['credit_score', 'tenure' ,'age','products_number', 'gender_Female','credit_card_0','active_member_0', 'churn_1']]

scaler = MinMaxScaler()
XS = scaler.fit_transform(df2)
X1 = pd.DataFrame(XS, columns= df2.columns)

X = X1.drop('churn_1', axis = 1)
y = X1['churn_1']

train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)
logmodel1 = LogisticRegression(solver = 'liblinear')
logmodel1.fit(train_X, train_y)
y_pred= logmodel1.predict(test_X)
print(confusion_matrix(test_y,y_pred))
print(classification_report(test_y,y_pred))
print('ROC AUC VALUE: ', roc_auc_score(test_y,logmodel1.predict_proba(test_X)[:,1]))
logit_model=sm.Logit(train_y, train_X)
logmodel_2=logit_model.fit()
print(logmodel_2.summary2())

#kNN model

scaler = MinMaxScaler()
XS = scaler.fit_transform(df1)
X1 = pd.DataFrame(XS, columns= df1.columns)

X = X1.drop('churn_1', axis = 1)
y = X1['churn_1']
train_X, test_X, train_y, test_y = train_test_split(X, y, test_size=0.3, random_state=1)

knn = KNeighborsClassifier(n_neighbors=15, metric='euclidean') #existen tres formas de medir las distancias una de ella es eucliden, manhattan, minkowski
knn.fit(train_X, train_y)

pred_y = knn.predict(test_X)

print(confusion_matrix(test_y,pred_y ))
print(classification_report(test_y,pred_y ))
print('ROC AUC VALUE: ', roc_auc_score(test_y,knn.predict_proba(test_X)[:,1]))

#kNN (optimize k)

maxK=100
cv_scores = []
for K in range(1,maxK):
  knn = KNeighborsClassifier(n_neighbors=K)
  scores = cross_val_score(knn,train_X,train_y.values.ravel(),cv = 5,scoring = "roc_auc")
  cv_scores.append(scores.mean())
  selected_k=cv_scores.index(max(cv_scores))+1

print('Optimal k: ', cv_scores.index(max(cv_scores))+1)

#ramdon forest

df3 = df1
X = df3.drop('churn_1', axis = 1)
y = df3['churn_1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

from sklearn.ensemble import RandomForestClassifier

rf_model = RandomForestClassifier(max_depth=5, random_state=0)
rf_model.fit(X_train,y_train)

y_pred_rf = rf_model.predict(X_test)

print(confusion_matrix(y_test,y_pred_rf))
print(classification_report(y_test,y_pred_rf))
print('ROC AUC VALUE: ', roc_auc_score(y_test,rf_model.predict_proba(X_test)[:,1]))

#Boosted Tree

df4 = df1
X = df4.drop('churn_1', axis = 1)
y = df4['churn_1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

from sklearn.ensemble import AdaBoostClassifier

bt_model = AdaBoostClassifier(n_estimators=100)

bt_model.fit(X_train,y_train)

y_pred_bt = bt_model.predict(X_test)

print(confusion_matrix(y_test,y_pred_bt))
print(classification_report(y_test,y_pred_bt))
print('ROC AUC:',roc_auc_score(y_test, bt_model.predict_proba(X_test)[:,1]))

#Naive Bayes

df5 = df1
X = df5.drop('churn_1', axis = 1)
y = df5['churn_1']

X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=1)

churn_nb = MultinomialNB(alpha=0.01)
churn_nb.fit(X_train, y_train)

y_pred = churn_nb.predict(X_test)

print(confusion_matrix(y_test,y_pred))
print(classification_report(y_test,y_pred))
print('ROC AUC:',roc_auc_score(y_test, churn_nb.predict_proba(X_test)[:,1]))

#ANN
scaler = MinMaxScaler()
XS = scaler.fit_transform(df1)
X1 = pd.DataFrame(XS, columns= df1.columns)

X = X1.drop('churn_1', axis = 1)
y = X1['churn_1']

X_train1, X_test1, y_train1, y_test1 = train_test_split(X,y,test_size=0.30,random_state=1)

MLPC_model = MLPClassifier(hidden_layer_sizes=6, activation='logistic', solver='adam', random_state=1)
MLPC_model.fit(X_train1, y_train1)
y_pred_mlpc1 = MLPC_model.predict(X_test1)

print(confusion_matrix(y_test1,y_pred_mlpc1))
print(classification_report(y_test1,y_pred_mlpc1))
print('ROC AUC VALUE: ', roc_auc_score(y_test1, MLPC_model.predict_proba(X_test1)[:,1]))

min_hidden_layer_size = 12
max_hidden_layer_size = 50
cv_scores = [ ]

for s in range(min_hidden_layer_size,max_hidden_layer_size,1):
    MLPC_model = MLPClassifier(hidden_layer_sizes=s, activation='logistic', solver='adam', random_state=1)
    scores = cross_val_score(MLPC_model,X_train,y_train.values.ravel(),cv = 5,scoring = "accuracy")
    cv_scores.append(scores.mean())
    selected_N=cv_scores.index(max(cv_scores))+6
print('Optimal N: ', cv_scores.index(max(cv_scores))+6)

#Keras
scaler = MinMaxScaler()
XS = scaler.fit_transform(df1)
X1 = pd.DataFrame(XS, columns= df1.columns)

X = X1.drop('churn_1', axis = 1)
y = X1['churn_1']

X_train, X_test, y_train, y_test = train_test_split(X,y,test_size=0.30,random_state=1)

model = Sequential()

model.add(Dense(units=15,activation='relu')) #layer 1
model.add(Dropout(0.5))
model.add(Dense(units=7,activation='relu')) #layer 2
model.add(Dropout(0.5))
model.add(Dense(units=1,activation='sigmoid')) #output

# For a binary classification problem
model.compile(loss='binary_crossentropy', optimizer='adam')

early_stop = EarlyStopping(monitor='val_loss', mode='min', verbose=1, patience=5)

print(classification_report(y_test,y_pred))
print(confusion_matrix(y_test,y_pred))
print('ROC AUC: ', roc_auc_score(y_test,model.predict(X_test)))

model.fit(x=X_train,
          y=y_train,
          batch_size=128,
          epochs=200,
          validation_data=(X_test, y_test), verbose=1,
          callbacks=[early_stop]
          )
model_loss = pd.DataFrame(model.history.history)
model_loss.plot()

# Logistic Regression
logit_roc_auc = roc_auc_score(test_y,logmodel.predict_proba(test_X)[:,1])
logit_fpr, logit_tpr, logit_thresholds = roc_curve(test_y, logmodel.predict_proba(test_X)[:,1])

#kNN
knn_roc_auc = roc_auc_score(test_y, knn.predict_proba(test_X)[:,1])
knn_fpr, knn_tpr, knn_thresholds= roc_curve(test_y, knn.predict_proba(test_X)[:,1])

# Random Forest
rf_roc_auc = roc_auc_score(y_test,rf_model.predict_proba(X_test)[:,1])
rf_fpr, rf_tpr, rf_thresholds = roc_curve(y_test,rf_model.predict_proba(X_test)[:,1])

# Boosted Tree
bt_roc_auc = roc_auc_score(y_test, bt_model.predict_proba(X_test)[:,1])
bt_fpr, bt_tpr, bt_thresholds = roc_curve(y_test, bt_model.predict_proba(X_test)[:,1])

#Naive Bayes
nb_roc_auc = roc_auc_score(y_test, churn_nb.predict_proba(X_test)[:,1])
nb_fpr, nb_tpr, nb_thresholds = roc_curve(y_test, churn_nb.predict_proba(X_test)[:,1])

#ANN
ann_roc_auc = roc_auc_score(y_test1, MLPC_model.predict_proba(X_test1)[:,1])
ann_fpr, ann_tpr, ann_thresholds = roc_curve(y_test1, MLPC_model.predict_proba(X_test1)[:,1])

#Keras
#krs_roc_auc = roc_auc_score(y_test,model.predict(X_test))
#krs_fpr, krs_tpr, krs_threshold = roc_auc_score(y_test,model.predict(X_test))


plt.figure()
plt.plot(logit_fpr, logit_tpr, label='Logistic Regression (area = %0.2f)' % logit_roc_auc)
plt.plot(knn_fpr, knn_tpr, label='Knn (area = %0.2f)' % knn_roc_auc)
plt.plot(rf_fpr, rf_tpr, label='Random Forest (area = %0.2f)' % rf_roc_auc)
plt.plot(bt_fpr, bt_tpr, label='Boosted tree (area = %0.2f)' % bt_roc_auc)
plt.plot(nb_fpr, nb_tpr, label='Naive Bayes (area = %0.2f)' % nb_roc_auc)
plt.plot(ann_fpr, ann_tpr, label='ANN (area = %0.2f)' % ann_roc_auc)
#plt.plot(ann_fpr, ann_tpr, label='Keras (area = %0.2f)' % krs_roc_auc)

plt.plot([0, 1], [0, 1],'r--')
plt.xlim([0.0, 1.0])
plt.ylim([0.0, 1.05])
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('Receiver operating characteristic')
plt.legend(loc="lower right")
plt.savefig('Log_ROC')
plt.show()